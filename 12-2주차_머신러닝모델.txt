강화학습은 안배움 - 게임데이터(업계에서는 많이 사용), 주식데이터에 한정이용됨 
근데 어려울것 


## 지도학습 :입출력이 있는 데이터 학습 
  스스로 예측할 수 있을때까지 계속 학습 
  (어린아이한테 설명해주는 것과 비슷)
 -> 제품 추천, 교통량 분석 가능 

- 분류(보통 이진분류와 삼진분류까지만 쓰임) 와 
 회귀(특징으로 값 예측 - 제대로 예측하기 쉽지 않아서 쓰기 어려움) 

## 비지도 학습 :정답이 없는 데이터를 특징끼리 군집화하여 결과 예측 
 - 데이터가 많이 필요한 학습. 아니면 편향될 수 있기 때문 
 -> 안면 인식, 유전자 서열분석, 시장조사, 사이버 보안등 
- 군집(clustering)
 cf. K-means clustering / hierarchical clustering / 

## 강화학습 :정답이 없고 자신이 한 행동 보상받으며 학습 -> 반복할수록 더 많은 상 받
 (머신러닝의 꽃이지만, 매우 어렵, 현실적 문제 해결하기도 까다롭) 


## 준지도학습 : 지도, 비지도 학습 둘다 적용할 수 없을때 (정답 데이터 충분하지 않을때)
		-> 라벨링 자동화





# 머신러닝 개발순서
1. import 
 (주의) 데이터 숫자인데 문자형데이터 -> 고객 key 00000000123 -> CSV -> 다운로드 -> import -> 문자형 -> 숫자형 123으로 바뀜
	그리고 한글컬럼 깨지는 경우 -> 인코딩 확인해야 
2. 데이터확인
3. 데이터 전처리
4. 데이터 분할 -> 평가 척도(예.정확도)로 데이터 나눔 

이제 데이터 준비됐으니
5. 알고리즘 선택 (예. Sklearn, XGBoost..), 비교         ###########
6. 학습 
7. 예측
8. 평가
9. 튜닝							###########여기가 데이터사이언티스트의 영역! 
 



피처 엔지니어링 -> 변수 개선?
하이퍼파라미터 최적화 -> 
성능검증 -> 

이런 부분들 면접갈때도 체크하면 좋을듯! 


- 프로젝트 할때도 체크리스트 확인하기!! 

- 프로세스 대해서 잘 정리해두기 
- 이런식으로 전처리, 비교 , 코드 .. 이런거 학습해두기 
- 프로세스 정립이 되어야 내일 실습때 빨리 나갈 수 있을것 




## KNN : 가장 가까운 이웃 찾는 알고리즘 
 = 지도학습 

 가깝다 기준? 거리 안에 데이터 빈도의 특성(3개라 지정하면)에 따라 

 *KNN 자료 더 찾아봐도됨 
 이웃의 개수는 홀수로 -> 짝수로 정할경우 다수결 따라 선택 어려움 


## 의사결정나무 - 랜덤포레스트 
 - 의사결정시, 전략모델(경영 컨설팅)로 많이 사용 
 - 현직자들이 좋아함 (직관적, 성능도 나쁘지 않고) 

트리모델 학습만 잘하게 될수도(복잡규칙, 과적합 문제) 보완 -> 앙상블 기법 등장 (단점 가진 모델들 결합하여 강한 모델 만듦)
결정트리(if-else같은)에서 의사결정나무 보완 한것 -> 랜덤 포레스트 

 - 랜더포레스트 : 훈련데이터에서 샘플 랜덤 추출 (단, 한 샘플 중복 추출가능) -> 좀 더 많은 데이터 학습위해
   		-> 는 전체 변수 선택 
1. 학습 데이터셋에서 무작위로 n개의 데이터셋 추출(중복허용)
2. 독립변수를 무작위로 n개 선택(중복 없음)
3. 추출 및 선택한 데이터와 변수로 의사결정나무 학습
4. step 1~3을 k번 반복
5. 생성한 k개의 의사결정나무를 통해 예측값을 산출하고, 분류나무는 가장 많이 분류한 범주로 예측하고, 회귀나무는 모든 나무의 평균값을 사용합니다.
	

## 앙상블 학습 
유형 중 보팅과 배깅은 투표로 최종 예측결과 결정하는 방식
 - 보팅 :서로 다른 알고리즘 투표  		     .. 의사결정나무
 - 배깅 :같은 알고리즘인데 데이터를 다르게 해서 투표 ..랜덤포레스트
 - 부스팅 :앞에서 학습한 예측 틀린 데이터에서 참고하여 다음 분류기에 가중치 부여 ..XGBoost, LightGBM




[연속형변수 예측]
-선형회귀 :시계열보다는 접근이 쉽 
	실제와 예측값 차이를 최소화하는 것에 초점 
	오차 줄이는 방식으로 튜닝, 학습됨 
	보통 10-15개 변수 사용 
 -> 업그레이드
 -릿지
 -라쏘
 -엘라스틱넷

[범주형변수 예측]
 -로지스틱 회귀 :피처(변수)는 줄이고, 계수 값의 크기를 조절한 방법론 



선형회귀는 값 예측 -> 따라서 쉽지않음
따라서 분류분석 모델을 많이 사용하게 될것 (..., 로지스틱회귀)

-> 의사결정나무에서 지니계수처럼 
로지스틱회귀도 오즈비와 로짓변환이 중요 



## 로지스틱회귀 : 0.5이상은 1, 0.5이하는 0으로 분류하는 방법론 (기준점 변경가능)
 -오즈 :사건 발생할 가능성이 않을 가능성보다 어느정도 큰지 -> 즉, 사람비교 머신러닝시 효과 얼마나 낼지 척도
       = 할 확률 / 발생않을 확률  

 예를들어, 100중 20개 target
 오즈비 = 

 feature -> 오즈비 -> 사람대비 성능 1.47배 높아짐 
 P값 < 0.05 일때 유의했었는데, 
 



## SVM (머신러닝 초창기 기법)
 - 회귀모델, 분류모델 둘다 사용가능 
 - 선형분류 불가능하면 -> 커널기법 (다항식커널, 가우시안커널(rbf))사용 

 속도 적고 + 옵션(파라미터) 적은데 -> 성능 좋음 
 
 C 높을수록 ->데이터 더 따라가서 -> 비선형
 감마 높을수록 -> 집단 특성 따라감 
 => 따라서 둘다 적당히 두어야 과적합이나 보수적 안할수 있음 -> 결정은 기계가 아니라 우리가 선택해야 
















